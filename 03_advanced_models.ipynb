{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX7-Dxv2oGVv",
        "outputId": "abb10c38-4e39-44e1-cad9-a0c05a2f6dd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/heartriskx/data/\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "heart2020 = pd.read_csv(base_path + \"heart_2020_clean.csv\")\n",
        "cardio = pd.read_csv(base_path + \"cardio_train_clean.csv\")\n",
        "uci = pd.read_csv(base_path + \"uci_cleveland_clean.csv\")\n"
      ],
      "metadata": {
        "id": "B2-Bvr28oh7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"HeartDisease\" in heart2020.columns:\n",
        "    heart2020 = heart2020.drop(columns=[\"HeartDisease\"])\n",
        "print(\"Heart2020 shape after dropping leakage col:\", heart2020.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzORW9jkomd9",
        "outputId": "204e4b27-d685-4bd7-d13c-8ada01a4b92c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heart2020 shape after dropping leakage col: (319795, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_models(X, y, dataset_name):\n",
        "    # One-hot encode categoricals\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "    # Fix column names for LightGBM (remove spaces/special chars)\n",
        "    X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
        "\n",
        "    # Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Scale for linear models\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    results[\"RandomForest\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # XGBoost\n",
        "    xgb = XGBClassifier(eval_metric='logloss', random_state=42, use_label_encoder=False)\n",
        "    xgb.fit(X_train, y_train)\n",
        "    y_pred = xgb.predict(X_test)\n",
        "    results[\"XGBoost\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # LightGBM\n",
        "    lgbm = LGBMClassifier(random_state=42)\n",
        "    lgbm.fit(X_train, y_train)\n",
        "    y_pred = lgbm.predict(X_test)\n",
        "    results[\"LightGBM\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Stacking Ensemble\n",
        "    estimators = [\n",
        "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "        ('xgb', XGBClassifier(eval_metric='logloss', random_state=42, use_label_encoder=False)),\n",
        "        ('lgbm', LGBMClassifier(random_state=42))\n",
        "    ]\n",
        "    stack = StackingClassifier(\n",
        "        estimators=estimators,\n",
        "        final_estimator=LogisticRegression(max_iter=1000),\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    stack.fit(X_train, y_train)\n",
        "    y_pred = stack.predict(X_test)\n",
        "    results[\"Stacking\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    print(f\"\\nðŸ“Š Advanced Results for {dataset_name}:\")\n",
        "    for model, metrics in results.items():\n",
        "        print(f\"{model}: Acc={metrics['accuracy']:.3f}, Prec={metrics['precision']:.3f}, \"\n",
        "              f\"Rec={metrics['recall']:.3f}, F1={metrics['f1']:.3f}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "IAhPTZmjpE4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heart2020\n",
        "X = heart2020.drop(columns=['target'])\n",
        "y = heart2020['target']\n",
        "res_heart2020_adv = run_models(X, y, \"Heart2020\")\n",
        "\n",
        "# Cardio\n",
        "X = cardio.drop(columns=['target', 'id'])\n",
        "y = cardio['target']\n",
        "res_cardio_adv = run_models(X, y, \"Cardio\")\n",
        "\n",
        "# UCI Cleveland\n",
        "X = uci.drop(columns=['target'])\n",
        "y = uci['target']\n",
        "res_uci_adv = run_models(X, y, \"UCI Cleveland\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9VdYaxmpHG1",
        "outputId": "bfa285bb-b2b4-4058-b0ec-213c4db9c36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:38:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 21898, number of negative: 233938\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060003 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 406\n",
            "[LightGBM] [Info] Number of data points in the train set: 255836, number of used features: 37\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.085594 -> initscore=-2.368661\n",
            "[LightGBM] [Info] Start training from score -2.368661\n",
            "\n",
            "ðŸ“Š Advanced Results for Heart2020:\n",
            "RandomForest: Acc=0.904, Prec=0.335, Rec=0.119, F1=0.176\n",
            "XGBoost: Acc=0.915, Prec=0.526, Rec=0.104, F1=0.174\n",
            "LightGBM: Acc=0.917, Prec=0.598, Rec=0.082, F1=0.145\n",
            "Stacking: Acc=0.915, Prec=0.523, Rec=0.146, F1=0.229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:43:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 27983, number of negative: 28017\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005422 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 714\n",
            "[LightGBM] [Info] Number of data points in the train set: 56000, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499696 -> initscore=-0.001214\n",
            "[LightGBM] [Info] Start training from score -0.001214\n",
            "\n",
            "ðŸ“Š Advanced Results for Cardio:\n",
            "RandomForest: Acc=0.713, Prec=0.719, Rec=0.701, F1=0.710\n",
            "XGBoost: Acc=0.732, Prec=0.751, Rec=0.694, F1=0.721\n",
            "LightGBM: Acc=0.734, Prec=0.753, Rec=0.698, F1=0.724\n",
            "Stacking: Acc=0.735, Prec=0.751, Rec=0.703, F1=0.726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:44:58] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 109, number of negative: 128\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000076 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 238\n",
            "[LightGBM] [Info] Number of data points in the train set: 237, number of used features: 13\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.459916 -> initscore=-0.160682\n",
            "[LightGBM] [Info] Start training from score -0.160682\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "ðŸ“Š Advanced Results for UCI Cleveland:\n",
            "RandomForest: Acc=0.867, Prec=0.885, Rec=0.821, F1=0.852\n",
            "XGBoost: Acc=0.867, Prec=0.885, Rec=0.821, F1=0.852\n",
            "LightGBM: Acc=0.817, Prec=0.870, Rec=0.714, F1=0.784\n",
            "Stacking: Acc=0.833, Prec=0.875, Rec=0.750, F1=0.808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Heart2020 (leakage fixed)\n",
        "X = heart2020.drop(columns=['target'])\n",
        "y = heart2020['target']\n",
        "res_heart2020_adv = run_models(X, y, \"Heart2020\")\n",
        "\n",
        "# Cardio\n",
        "X = cardio.drop(columns=['target', 'id'])\n",
        "y = cardio['target']\n",
        "res_cardio_adv = run_models(X, y, \"Cardio\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b1P72kpzQNV",
        "outputId": "33deef67-bdd1-4a94-873f-80431567d3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [15:20:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 21898, number of negative: 233938\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059442 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 406\n",
            "[LightGBM] [Info] Number of data points in the train set: 255836, number of used features: 37\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.085594 -> initscore=-2.368661\n",
            "[LightGBM] [Info] Start training from score -2.368661\n",
            "\n",
            "ðŸ“Š Advanced Results for Heart2020:\n",
            "RandomForest: Acc=0.904, Prec=0.335, Rec=0.119, F1=0.176\n",
            "XGBoost: Acc=0.915, Prec=0.526, Rec=0.104, F1=0.174\n",
            "LightGBM: Acc=0.917, Prec=0.598, Rec=0.082, F1=0.145\n",
            "Stacking: Acc=0.915, Prec=0.523, Rec=0.146, F1=0.229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [15:25:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 27983, number of negative: 28017\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008679 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 714\n",
            "[LightGBM] [Info] Number of data points in the train set: 56000, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499696 -> initscore=-0.001214\n",
            "[LightGBM] [Info] Start training from score -0.001214\n",
            "\n",
            "ðŸ“Š Advanced Results for Cardio:\n",
            "RandomForest: Acc=0.713, Prec=0.719, Rec=0.701, F1=0.710\n",
            "XGBoost: Acc=0.732, Prec=0.751, Rec=0.694, F1=0.721\n",
            "LightGBM: Acc=0.734, Prec=0.753, Rec=0.698, F1=0.724\n",
            "Stacking: Acc=0.735, Prec=0.751, Rec=0.703, F1=0.726\n"
          ]
        }
      ]
    }
  ]
}
