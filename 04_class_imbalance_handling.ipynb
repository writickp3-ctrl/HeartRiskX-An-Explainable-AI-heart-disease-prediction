{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujH-BSkpBv9D",
        "outputId": "129f5b49-61e6-467a-a9d4-e50b69e429ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/heartriskx/data/\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# For imbalance handling\n",
        "from imblearn.over_sampling import SMOTE\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "heart2020 = pd.read_csv(base_path + \"heart_2020_clean.csv\")\n",
        "\n",
        "if \"HeartDisease\" in heart2020.columns:\n",
        "    heart2020 = heart2020.drop(columns=[\"HeartDisease\"])  # just in case\n",
        "print(heart2020['target'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzvy6msFCMbT",
        "outputId": "0d1283b2-b5cd-490a-8dcb-af8585a43215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target\n",
            "0    292422\n",
            "1     27373\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_balanced_baselines(X, y, dataset_name):\n",
        "    # One-hot encode categoricals\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "    X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Balance only training set (not test set!)\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(f\"\\nðŸ”„ {dataset_name}: Before balancing: {y_train.value_counts().to_dict()}, After balancing: {y_train_bal.value_counts().to_dict()}\")\n",
        "\n",
        "    # Scale\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_bal)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Logistic Regression\n",
        "    logreg = LogisticRegression(max_iter=1000)\n",
        "    logreg.fit(X_train_scaled, y_train_bal)\n",
        "    y_pred = logreg.predict(X_test_scaled)\n",
        "    results[\"LogReg\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "    rf.fit(X_train_bal, y_train_bal)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    results[\"RandomForest\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    print(f\"\\nðŸ“Š Balanced Baseline Results for {dataset_name}:\")\n",
        "    for model, metrics in results.items():\n",
        "        print(f\"{model}: Acc={metrics['accuracy']:.3f}, Prec={metrics['precision']:.3f}, \"\n",
        "              f\"Rec={metrics['recall']:.3f}, F1={metrics['f1']:.3f}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "oKRAzjRzCb27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = heart2020.drop(columns=['target'])\n",
        "y = heart2020['target']\n",
        "res_heart2020_balanced = run_balanced_baselines(X, y, \"Heart2020 (Balanced)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHKHvkEcCeEz",
        "outputId": "6bca1f77-827a-4113-ec4c-c9ff6115f6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”„ Heart2020 (Balanced): Before balancing: {0: 233938, 1: 21898}, After balancing: {0: 233938, 1: 233938}\n",
            "\n",
            "ðŸ“Š Balanced Baseline Results for Heart2020 (Balanced):\n",
            "LogReg: Acc=0.841, Prec=0.272, Rec=0.513, F1=0.355\n",
            "RandomForest: Acc=0.878, Prec=0.295, Rec=0.309, F1=0.302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "def run_balanced_advanced(X, y, dataset_name):\n",
        "    # One-hot encode categoricals\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "    X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Balance only training set (not test set!)\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(f\"\\nðŸ”„ {dataset_name}: Before balancing: {y_train.value_counts().to_dict()}, After balancing: {y_train_bal.value_counts().to_dict()}\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "    rf.fit(X_train_bal, y_train_bal)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    results[\"RandomForest\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # XGBoost\n",
        "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "    xgb.fit(X_train_bal, y_train_bal)\n",
        "    y_pred = xgb.predict(X_test)\n",
        "    results[\"XGBoost\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # LightGBM\n",
        "    lgbm = lgb.LGBMClassifier(random_state=42)\n",
        "    lgbm.fit(X_train_bal, y_train_bal)\n",
        "    y_pred = lgbm.predict(X_test)\n",
        "    results[\"LightGBM\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Stacking (LogReg + RF + XGB + LGBM)\n",
        "    stack = StackingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', rf),\n",
        "            ('xgb', xgb),\n",
        "            ('lgbm', lgbm)\n",
        "        ],\n",
        "        final_estimator=LogisticRegression(max_iter=1000),\n",
        "        passthrough=True\n",
        "    )\n",
        "    stack.fit(X_train_bal, y_train_bal)\n",
        "    y_pred = stack.predict(X_test)\n",
        "    results[\"Stacking\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    print(f\"\\nðŸ“Š Balanced Advanced Results for {dataset_name}:\")\n",
        "    for model, metrics in results.items():\n",
        "        print(f\"{model}: Acc={metrics['accuracy']:.3f}, Prec={metrics['precision']:.3f}, \"\n",
        "              f\"Rec={metrics['recall']:.3f}, F1={metrics['f1']:.3f}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "0CHxNqL5EIHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = heart2020.drop(columns=['target'])\n",
        "y = heart2020['target']\n",
        "res_heart2020_adv_balanced = run_balanced_advanced(X, y, \"Heart2020 (Balanced)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhGImDp6ER8D",
        "outputId": "34af2895-a0c1-4029-dfcf-fa46c2f3e2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”„ Heart2020 (Balanced): Before balancing: {0: 233938, 1: 21898}, After balancing: {0: 233938, 1: 233938}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:35:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 233938, number of negative: 233938\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.336436 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1086\n",
            "[LightGBM] [Info] Number of data points in the train set: 467876, number of used features: 37\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:39:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 233938, number of negative: 233938\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.260971 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1086\n",
            "[LightGBM] [Info] Number of data points in the train set: 467876, number of used features: 37\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:55:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:55:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:55:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:55:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:56:02] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 187150, number of negative: 187150\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197232 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1086\n",
            "[LightGBM] [Info] Number of data points in the train set: 374300, number of used features: 37\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 187151, number of negative: 187150\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202488 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1086\n",
            "[LightGBM] [Info] Number of data points in the train set: 374301, number of used features: 37\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000005\n",
            "[LightGBM] [Info] Start training from score 0.000005\n",
            "[LightGBM] [Info] Number of positive: 187151, number of negative: 187150\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.280141 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1086\n",
            "[LightGBM] [Info] Number of data points in the train set: 374301, number of used features: 37\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000005\n",
            "[LightGBM] [Info] Start training from score 0.000005\n",
            "[LightGBM] [Info] Number of positive: 187150, number of negative: 187151\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200168 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1086\n",
            "[LightGBM] [Info] Number of data points in the train set: 374301, number of used features: 37\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000005\n",
            "[LightGBM] [Info] Start training from score -0.000005\n",
            "[LightGBM] [Info] Number of positive: 187150, number of negative: 187151\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203618 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1086\n",
            "[LightGBM] [Info] Number of data points in the train set: 374301, number of used features: 37\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000005\n",
            "[LightGBM] [Info] Start training from score -0.000005\n",
            "\n",
            "ðŸ“Š Balanced Advanced Results for Heart2020 (Balanced):\n",
            "RandomForest: Acc=0.878, Prec=0.295, Rec=0.309, F1=0.302\n",
            "XGBoost: Acc=0.867, Prec=0.296, Rec=0.402, F1=0.341\n",
            "LightGBM: Acc=0.858, Prec=0.291, Rec=0.462, F1=0.357\n",
            "Stacking: Acc=0.885, Prec=0.308, Rec=0.280, F1=0.294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure UCI target is binary (0 = no disease, 1 = disease)\n",
        "uci['target'] = (uci['target'] > 0).astype(int)\n",
        "\n",
        "# Now run models\n",
        "X = uci.drop(columns=['target'])\n",
        "y = uci['target']\n",
        "res_uci_cs = run_cost_sensitive_models(X, y, \"UCI Cleveland (Cost-sensitive)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3m5fLVTR6Vr",
        "outputId": "45d9f3a5-deac-4ec2-b16b-768184d194f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 111, number of negative: 130\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000116 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 239\n",
            "[LightGBM] [Info] Number of data points in the train set: 241, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "âš¡ Best F1 for UCI Cleveland (Cost-sensitive) (RF) = 0.933 at threshold=0.440 (Prec=0.875, Rec=1.000)\n",
            "\n",
            "ðŸ“Š Cost-sensitive Results for UCI Cleveland (Cost-sensitive):\n",
            "LogReg: Acc=0.951, Prec=0.963, Rec=0.929, F1=0.945, ROC-AUC=0.994, PR-AUC=0.992\n",
            "RandomForest: Acc=0.902, Prec=0.893, Rec=0.893, F1=0.893, ROC-AUC=0.976, PR-AUC=0.971\n",
            "XGBoost: Acc=0.820, Prec=0.815, Rec=0.786, F1=0.800, ROC-AUC=0.937, PR-AUC=0.920\n",
            "LightGBM: Acc=0.902, Prec=0.867, Rec=0.929, F1=0.897, ROC-AUC=0.968, PR-AUC=0.959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
        "\n",
        "def run_cost_sensitive_models(X, y, dataset_name):\n",
        "    # One-hot encode categoricals if present\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "    X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Logistic Regression\n",
        "    logreg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "    logreg.fit(X_train, y_train)\n",
        "    proba = logreg.predict_proba(X_test)[:,1]\n",
        "    y_pred = logreg.predict(X_test)\n",
        "    results[\"LogReg\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred),\n",
        "        \"roc_auc\": roc_auc_score(y_test, proba),\n",
        "        \"pr_auc\": average_precision_score(y_test, proba)\n",
        "    }\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=300, random_state=42, class_weight='balanced')\n",
        "    rf.fit(X_train, y_train)\n",
        "    proba = rf.predict_proba(X_test)[:,1]\n",
        "    y_pred = rf.predict(X_test)\n",
        "    results[\"RandomForest\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred),\n",
        "        \"roc_auc\": roc_auc_score(y_test, proba),\n",
        "        \"pr_auc\": average_precision_score(y_test, proba)\n",
        "    }\n",
        "\n",
        "    # XGBoost\n",
        "    xgb = XGBClassifier(\n",
        "        eval_metric='logloss',\n",
        "        random_state=42,\n",
        "        scale_pos_weight=(y_train.value_counts()[0] / y_train.value_counts()[1])\n",
        "    )\n",
        "    xgb.fit(X_train, y_train)\n",
        "    proba = xgb.predict_proba(X_test)[:,1]\n",
        "    y_pred = xgb.predict(X_test)\n",
        "    results[\"XGBoost\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred),\n",
        "        \"roc_auc\": roc_auc_score(y_test, proba),\n",
        "        \"pr_auc\": average_precision_score(y_test, proba)\n",
        "    }\n",
        "\n",
        "    # LightGBM\n",
        "    lgbm = LGBMClassifier(random_state=42, class_weight='balanced')\n",
        "    lgbm.fit(X_train, y_train)\n",
        "    proba = lgbm.predict_proba(X_test)[:,1]\n",
        "    y_pred = lgbm.predict(X_test)\n",
        "    results[\"LightGBM\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred),\n",
        "        \"recall\": recall_score(y_test, y_pred),\n",
        "        \"f1\": f1_score(y_test, y_pred),\n",
        "        \"roc_auc\": roc_auc_score(y_test, proba),\n",
        "        \"pr_auc\": average_precision_score(y_test, proba)\n",
        "    }\n",
        "\n",
        "    # Threshold tuning for RF\n",
        "    proba = rf.predict_proba(X_test)[:,1]\n",
        "    prec, rec, th = precision_recall_curve(y_test, proba)\n",
        "    f1 = 2 * prec*rec / (prec+rec + 1e-9)\n",
        "    best = f1.argmax()\n",
        "    print(f\"\\nâš¡ Best F1 for {dataset_name} (RF) = {f1[best]:.3f} at threshold={th[best]:.3f} \"\n",
        "          f\"(Prec={prec[best]:.3f}, Rec={rec[best]:.3f})\")\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nðŸ“Š Cost-sensitive Results for {dataset_name}:\")\n",
        "    for model, metrics in results.items():\n",
        "        print(f\"{model}: Acc={metrics['accuracy']:.3f}, Prec={metrics['precision']:.3f}, \"\n",
        "              f\"Rec={metrics['recall']:.3f}, F1={metrics['f1']:.3f}, \"\n",
        "              f\"ROC-AUC={metrics['roc_auc']:.3f}, PR-AUC={metrics['pr_auc']:.3f}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "pgWWW2DyPm9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload datasets from Drive\n",
        "cardio = pd.read_csv(base_path + \"cardio_train.csv\", sep=';')\n",
        "cardio.rename(columns={'cardio': 'target'}, inplace=True)\n",
        "\n",
        "heart2020 = pd.read_csv(base_path + \"heart_2020.csv\")\n",
        "\n",
        "uci = pd.read_csv(base_path + \"uci_heart.csv\")   # <-- use this filename\n",
        "uci.rename(columns={uci.columns[-1]: 'target'}, inplace=True)\n"
      ],
      "metadata": {
        "id": "opbQOaQVQYRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix UCI target to binary\n",
        "uci['target'] = uci['target'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Now run\n",
        "X = uci.drop(columns=['target'])\n",
        "y = uci['target']\n",
        "res_uci_cs = run_cost_sensitive_models(X, y, \"UCI Cleveland (Cost-sensitive)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNfdXZ8iTDsi",
        "outputId": "915c35d1-816f-4031-985f-42ec602ea86a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 111, number of negative: 130\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000110 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 239\n",
            "[LightGBM] [Info] Number of data points in the train set: 241, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "âš¡ Best F1 for UCI Cleveland (Cost-sensitive) (RF) = 0.933 at threshold=0.440 (Prec=0.875, Rec=1.000)\n",
            "\n",
            "ðŸ“Š Cost-sensitive Results for UCI Cleveland (Cost-sensitive):\n",
            "LogReg: Acc=0.951, Prec=0.963, Rec=0.929, F1=0.945, ROC-AUC=0.994, PR-AUC=0.992\n",
            "RandomForest: Acc=0.902, Prec=0.893, Rec=0.893, F1=0.893, ROC-AUC=0.976, PR-AUC=0.971\n",
            "XGBoost: Acc=0.820, Prec=0.815, Rec=0.786, F1=0.800, ROC-AUC=0.937, PR-AUC=0.920\n",
            "LightGBM: Acc=0.902, Prec=0.867, Rec=0.929, F1=0.897, ROC-AUC=0.968, PR-AUC=0.959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run cost-sensitive models on Cardio\n",
        "X = cardio.drop(columns=['target', 'id'])\n",
        "y = cardio['target']\n",
        "res_cardio_cs = run_cost_sensitive_models(X, y, \"Cardio (Cost-sensitive)\")\n",
        "\n",
        "# Run cost-sensitive models on UCI\n",
        "X = uci.drop(columns=['target'])\n",
        "y = uci['target']\n",
        "res_uci_cs = run_cost_sensitive_models(X, y, \"UCI Cleveland (Cost-sensitive)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8sEQowaPrqC",
        "outputId": "b1264f82-caee-4f26-f3a4-349b164d50fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 27983, number of negative: 28017\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006324 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 714\n",
            "[LightGBM] [Info] Number of data points in the train set: 56000, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "\n",
            "âš¡ Best F1 for Cardio (Cost-sensitive) (RF) = 0.724 at threshold=0.330 (Prec=0.638, Rec=0.837)\n",
            "\n",
            "ðŸ“Š Cost-sensitive Results for Cardio (Cost-sensitive):\n",
            "LogReg: Acc=0.704, Prec=0.720, Rec=0.667, F1=0.693, ROC-AUC=0.763, PR-AUC=0.743\n",
            "RandomForest: Acc=0.714, Prec=0.718, Rec=0.704, F1=0.711, ROC-AUC=0.772, PR-AUC=0.754\n",
            "XGBoost: Acc=0.730, Prec=0.749, Rec=0.691, F1=0.718, ROC-AUC=0.794, PR-AUC=0.775\n",
            "LightGBM: Acc=0.736, Prec=0.753, Rec=0.701, F1=0.726, ROC-AUC=0.800, PR-AUC=0.781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 111, number of negative: 130\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000111 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 239\n",
            "[LightGBM] [Info] Number of data points in the train set: 241, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "âš¡ Best F1 for UCI Cleveland (Cost-sensitive) (RF) = 0.933 at threshold=0.440 (Prec=0.875, Rec=1.000)\n",
            "\n",
            "ðŸ“Š Cost-sensitive Results for UCI Cleveland (Cost-sensitive):\n",
            "LogReg: Acc=0.951, Prec=0.963, Rec=0.929, F1=0.945, ROC-AUC=0.994, PR-AUC=0.992\n",
            "RandomForest: Acc=0.902, Prec=0.893, Rec=0.893, F1=0.893, ROC-AUC=0.976, PR-AUC=0.971\n",
            "XGBoost: Acc=0.820, Prec=0.815, Rec=0.786, F1=0.800, ROC-AUC=0.937, PR-AUC=0.920\n",
            "LightGBM: Acc=0.902, Prec=0.867, Rec=0.929, F1=0.897, ROC-AUC=0.968, PR-AUC=0.959\n"
          ]
        }
      ]
    }
  ]
}
